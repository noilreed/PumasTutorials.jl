---
title: Exercise PK39 - Fitting Two Compartment data - Experimental design issues
date: `j Date(now())`
---

```julia; echo = false
using Dates
```

## Objectives

In this exercise we will learn how to analyze the same dataset with 24 observations,
 14 observations and 9 observations. This will help us to understand if **extra data
 provides any better information**. Finally we will conclude at the number of datapoints
 to be used for analysis until the information is `lost`.

The basic workflow for the estimation process is:

 1. Description of the data
 2. Exploratory analysis of the data
 3. NCA Analysis
 4. Pharmacokinetic modelling
 5. Diagnostic Plots
 6. Validation

Lets load the necessary `libraries` before we get started

```julia
using PumasTutorials
using Random
using CSV
using Pumas
using Plots
using StatsPlots
using Pipe
using StatsBase
using PrettyTables
```

## Description of the data

In this analysis the drug is given as three infusions. Details and doses of the
 infusions are shown below. PK samples are collected at various time points `0.25,
  0.5, 1, 2, 3, 6, 8, 9, 10, 12, 18, 21, 24, 24.5, 25, 26, 28, 30, 32, 34, 36, 42,
  48, 60 hrs`.

```julia
Dosage = DataFrame(- = ["Dose (μg/kg)", "Time Interval"],
 First_Dose = ["26.9", "15 min"], Second_Dose = ["139", "15min - 8hrs"],
 Third_Dose = ["138.95", "8hrs to 24hrs"], Total_Dose=["304.85", "-"])
pretty_table(first(Dosage,10), backend = :html)
```

The following are the units of the dataset:

 * Time (time) = hrs
 * Plasma Concentration (dv) = μg/L
 * Dose (amt) = μg/kg

```julia
pk39_data_df = CSV.read("./pk_39.csv", DataFrame, missingstrings = ["", ".", "NA", "BQL"])
```

Basic summary `statistics` of the data

```julia
stats_pk39_data = describe(pk39_data_df, :min, :max, :mean, :std, :nmissing, cols=[:id,:time,:dv])
```

## Exploratory Plots of the given data

 * Plot of Concentration vs Time

```julia
pk_data_plot = dropmissing(pk39_data_df, :dv)
@df pk_data_plot plot(:time, :dv, group=:id, label=false,
                      xlabel="Time (hr)", ylabel="Concentration (ug/L)",
                      size = (600, 600), guidefontsize = 12,
                      title = "Concentration vs Time")
```

## NCA Analysis

We will perform an `NCA Analysis` to obtain the _AUC_ of the given data. Once we
 have the AUC we will calculate the other parameters for our intial estimates for
 fitting. From the above given data we will select only the columns we require
 for the NCA analysis.

```julia; results="hidden"
pk_data_nca = dropmissing(pk39_data_df, :dv)
select!(pk_data_nca, :id, :time, :dv)
```

Now, map the data variables to the read_nca function that prepares the data for
 NCA analysis. You can even type **?read_nca** in the REPL and get more information
 on the mapping of the data.

```julia
pk39_nca = read_nca(pk_data_nca,
                    id       = :id,
                    time     = :time,
                    conc     = :dv)
```

A full NCAReport is generated, we will then perform summary statistics of the
 required parameters to obtain the **Mean, GeometricMean, and SD**

```julia
pk39_nca_report = NCAReport(pk39_nca, sigdig=3)
```

Perform the Summary `Statistics` for the required NCA Parameters

```julia
## Select the required parameters from the NCA Report
stats_nca_df = select(pk39_nca_report, [:id, :kel, :half_life, :aucinf_obs, :cmax])

## Stack the data for easy computation
stats_nca_stacked = stack(stats_nca_df, [:kel, :half_life, :aucinf_obs, :cmax], [:id])
stats_nca_summary = combine(groupby(stats_nca_stacked,[:variable]),
                            [col => fun for col in [:value]
                            for fun in [mean, geomean, std]])
```

* We have the AUC and Total Dose from which we will calculate the Clearance using
   the formula _Cl = Total Dose / AUC_, i.e **Cl = 0.41 L/kg/hr**
* The Intercompartmental clearance is assumed to be greater than the Clearance
   because of the steep decline in concentration i.e **Q = 1.0 L/kg/hr**
* The Volume of Central Compartment can be calculated as _Doseiv/Cpeak_, i.e
   **Vc = 0.5 L**
* We assume the Volume of the Peripheral Compartment (Vp) to be more than Vc.
   Hence, we will use a value of **Vp=1.2 L** for the initial estimate.

## Pharmacokinetic Modelling

##### Read the dataset into read_pumas()

In the initial analysis we will use the full dataset consiting of **24 observation**

```julia
pk39_data = read_pumas(pk39_data_df,
                        id           = :id,
                        time         = :time,
                        observations = [:dv],
                        amt          = :amt,
                        evid         = :evid,
                        cmt          = :cmt,
                        rate         = :rate)
```

##### Two-Compartment Model

```julia
pk_39           = @emmodel begin
  @metadata begin
    desc = "PK39 Two Compartment Model"
    timeu = u"hr"
  end

  @random begin
    "Clearance (L/hr)"
    CL          ~ 1 | LogNormal 
    "Volume (L)"
    Vc          ~ 1 | LogNormal
    "Peripheral Volume (L)"
    Vp          ~ 1 | LogNormal
    "Intercompartmental Clearance (L/hr)"
    Q           ~ 1 | LogNormal
  end

  @dynamics Central1Periph1
    #Central'    =  (Q/Vp)*Peripheral - (Q/Vc)*Central -(CL/Vc)*Central
    #Peripheral' = -(Q/Vp)*Peripheral + (Q/Vc)*Central
  #end

  @post begin
    cp          = Central/Vc
  end

  @error begin
    dv          ~ ProportionalNormal(cp)
  end
end
```

We have obtained the initial estimates from `NCA` and few other calculations.

```julia
param_est = (CL    = 0.41,
             Vc    = 0.5,
             Vp    = 1.2,
             Q     = 1.0)
```

##### SAEM Fitting

We will now try to fit a two-compartment model using `SAEM`

```julia
pk_39_fit = @time fit(pk_39, pk39_data, param_est,
                    Pumas.SAEM(), ensemblealg=EnsembleThreads())

coeftable(pk_39_fit)
```


We will now perform the similar analysis using only **14 observations** from the
 previous dataset. Using only the 14 observations we will filter and then read the
 dataset into _read_pumas()_

```julia
pk39_data_df_14 = filter(x -> x.time in [0,0.25,1,3,8,9,12,18,24,25,28,32,36,48,60], pk39_data_df)

pk39_data_14 = read_pumas(pk39_data_df_14,
                          id           = :id,
                          time         = :time,
                          observations = [:dv],
                          amt          = :amt,
                          evid         = :evid,
                          cmt          = :cmt,
                          rate         = :rate)
```

We will try to fit the 14 observations using `SAEM`

```julia
pk_39_fit_14 = @time fit(pk_39, pk39_data_14, param_est,
                    Pumas.SAEM(), ensemblealg=EnsembleThreads())

coeftable(pk_39_fit_14)
```

